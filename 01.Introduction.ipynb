{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#marp: true\n",
    "#theme: gaia\n",
    "#paginate: true\n",
    "---\n",
    "\n",
    "# Введение в классическое NLP  \n",
    "## Основные задачи обработки естественного языка\n",
    "\n",
    "---\n",
    "\n",
    "## Морфологический анализ\n",
    "\n",
    "Работа с морфологическими характеристиками отдельных слов.  \n",
    "\n",
    "- **Стемминг**  \n",
    "  Отбрасывание суффиксов/окончаний для получения основы слова.  \n",
    "  > *running → run, played → play*  \n",
    "\n",
    "- **Лемматизация**  \n",
    "  Приведение слова к нормальной форме.  \n",
    "  > *мыши → мышь, children → child*  \n",
    "\n",
    "- **Частеречная разметка (POS-tagging)**  \n",
    "  Определение части речи и грамматических признаков.  \n",
    "  > *book (сущ.) vs. book (глаг.)*  \n",
    "\n",
    "- **Морфологическая сегментация**  \n",
    "  Разделение слова на морфемы (*prefix-root-suffix*).  \n",
    "  > *unhappiness → un + happy + ness*  \n",
    "\n",
    "---\n",
    "\n",
    "## Синтаксический анализ\n",
    "\n",
    "- **Определение границ предложений**  \n",
    "  Где заканчивается предложение.  \n",
    "  > *Я люблю NLP. Оно интересно.*  \n",
    "\n",
    "- **Парсинг предложений**  \n",
    "  - **Дерево зависимостей** (связь между словами)  \n",
    "    ![Дерево зависимостей](./img/dependency_tree.png)  \n",
    "  - **Синтаксическое дерево** (структура вложенности)  \n",
    "    ![Синтаксическое дерево](./img/constituency_tree.png)  \n",
    "\n",
    "- **Синтаксические роли**  \n",
    "  > *Кошка (подлежащее) ест (сказуемое) рыбу (дополнение).*  \n",
    "\n",
    "---\n",
    "\n",
    "## Свойства предложений\n",
    "\n",
    "- **Извлечение именованных сущностей (NER)**  \n",
    "  > *Барак Обама был президентом США.*  \n",
    "  → *Барак Обама = PERSON, США = LOCATION*  \n",
    "\n",
    "- **Разрешение лексической многозначности (WSD)**  \n",
    "  > *Я потерял ключ.* (металлический предмет)  \n",
    "  > *Ключ к задаче найден.* (решение)  \n",
    "\n",
    "- **Entity Linking (связывание сущностей)**  \n",
    "  > *Ливерпуль выиграл чемпионат.*  \n",
    "  → Связь с футбольным клубом, а не с городом.  \n",
    "\n",
    "- **Выявление событий**  \n",
    "  > *Землетрясение произошло в Японии в 2011 году.*  \n",
    "\n",
    "---\n",
    "\n",
    "## Семантический анализ\n",
    "\n",
    "- **Семантическое представление предложений**  \n",
    "  Преобразование в логическую форму:  \n",
    "  > *Каждый студент любит математику*  \n",
    "  → ∀x (Student(x) → Likes(x, Math))  \n",
    "\n",
    "- **Извлечение отношений между сущностями**  \n",
    "  > *Илон Маск основал SpaceX.*  \n",
    "  → (Илон Маск, founder_of, SpaceX)  \n",
    "\n",
    "- **Semantic Role Labeling (SRL)**  \n",
    "  Определение «кто сделал что с кем».  \n",
    "  > *Маша (агент) подарила (действие) книгу (тема) Петру (бенефициар).*  \n",
    "\n",
    "- **Онтологии и базы знаний**  \n",
    "  Связь текста с Wikidata, DBpedia, WordNet.  \n",
    "\n",
    "---\n",
    "\n",
    "## Анализ на уровне текста\n",
    "\n",
    "- **Классификация текстов**  \n",
    "  > новость → *спорт / политика / экономика*  \n",
    "\n",
    "- **Кластеризация текстов**  \n",
    "  > grouping: «футбол», «баскетбол» → *спорт*  \n",
    "\n",
    "- **Coreference resolution**  \n",
    "  > *Маша купила книгу. Она ей понравилась.*  \n",
    "  → *Она* = Книга  \n",
    "\n",
    "- **Тематическая сегментация**  \n",
    "  > Википедия: *география / климат / население*  \n",
    "\n",
    "- **Анализ тональности (Sentiment Analysis)**  \n",
    "  > *Фильм был потрясающим!* → позитив  \n",
    "  > *Мне не нравится этот напиток.* → негатив  \n",
    "\n",
    "- **Дискурс-анализ**  \n",
    "  Определение логической связи между предложениями  \n",
    "  (причина, следствие, контраст).  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Морфологическая классификация языков\n",
    "\n",
    "---\n",
    "\n",
    "## Аналитические языки  \n",
    "\n",
    "Грамматические отношения передаются в основном через **синтаксис** и служебные слова.  \n",
    "\n",
    "Характеристика:  \n",
    "- мало морфем в слове  \n",
    "- порядок слов играет ключевую роль  \n",
    "\n",
    "Примеры:  \n",
    "- **Английский**: *I will go to the park*  \n",
    "- **Китайский**: 他去公园 (*он идти парк*)  \n",
    "\n",
    "---\n",
    "\n",
    "## Синтетические языки  \n",
    "\n",
    "Грамматические отношения выражаются при помощи **морфем**.  \n",
    "Высокое соотношение *морфем к слову*.  \n",
    "\n",
    "Подтипы:  \n",
    "1. Флективные  \n",
    "2. Агглютинативные  \n",
    "3. Полисинтетические  \n",
    "\n",
    "---\n",
    "\n",
    "## Флективные языки  \n",
    "\n",
    "- Используются **флексии** (окончания, изменения слова)  \n",
    "- Одна морфема может выражать несколько грамматических категорий одновременно  \n",
    "\n",
    "Примеры:  \n",
    "- **Русский**: *дом → дома → домов*  \n",
    "- **Латинский**: *amicus → amici → amico*  \n",
    "\n",
    "---\n",
    "\n",
    "## Агглютинативные языки  \n",
    "\n",
    "- Слова образуются путем добавления морфем-«кирпичиков»  \n",
    "- Каждая морфема выражает одну грамматическую категорию  \n",
    "\n",
    "Примеры:  \n",
    "- **Турецкий**: *ev* (дом) → *evler* (дома) → *evlerimizde* (в наших домах)  \n",
    "- **Финский**: *talo* (дом) → *talossa* (в доме)  \n",
    "\n",
    "---\n",
    "\n",
    "## Полисинтетические языки  \n",
    "\n",
    "- Очень высокое соотношение морфем к слову  \n",
    "- Целое предложение может быть выражено одним словом  \n",
    "\n",
    "Примеры:  \n",
    "- **Чукотский**: *təmeyŋəlevtpəγtərkən* = «у меня сильно болит голова»  \n",
    "- **Инуитские языки**\n",
    "\n",
    "---\n",
    "\n",
    "# Выводы  \n",
    "\n",
    "- Аналитические языки → больше опоры на синтаксис  \n",
    "- Синтетические языки → больше морфем внутри слова  \n",
    "- Внутри синтетических:  \n",
    "  - флективные → окончания (русский, латинский)  \n",
    "  - агглютинативные → «цепочка морфем» (турецкий, финский)  \n",
    "  - полисинтетические → «слово-предложение» (инуитские, чукотский)  \n",
    "\n",
    "---\n",
    "\n",
    "## Прикладные задачи NLP\n",
    "\n",
    "- **Машинный перевод**  \n",
    "  > Google Translate, DeepL  \n",
    "\n",
    "- **Распознавание речи**  \n",
    "  > Siri, Alexa, Яндекс.Алиса  \n",
    "\n",
    "- **Суммаризация текста**  \n",
    "  > Сжатие длинной статьи до краткого содержания  \n",
    "\n",
    "- **Корректировка ошибок**  \n",
    "  > *hte → the, recieve → receive*  \n",
    "\n",
    "- **Автодополнение текста**  \n",
    "  > Gmail Smart Compose  \n",
    "\n",
    "- **Диалоговые системы**  \n",
    "  > ChatGPT, чат-боты поддержки  \n",
    "\n",
    "- **Вопрос-ответные системы**  \n",
    "  > *Когда родился Пушкин? → 1799 г.*  \n",
    "\n",
    "- **Информационный поиск (IR)**  \n",
    "  > Google Search, Яндекс  \n",
    "\n",
    "- **Генерация по тексту**  \n",
    "  > MidJourney, DALL·E, MusicLM  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Языковые модели\n",
    "\n",
    "*Языковая модель* — вероятностная модель, которая присваивает вероятность последовательности слов  \n",
    "\n",
    "\n",
    "$$\\boldsymbol{w} = w_1, w_2, \\ldots, w_n$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Задачи языковой модели  \n",
    "\n",
    "Оценить вероятность текста:  \n",
    "\n",
    "$$P(w_1, w_2, \\ldots, w_n)$$\n",
    "\n",
    "Примеры:  \n",
    "- *Я иду домой* → высокая вероятность  \n",
    "- *Домой иду я* → допустимо, но реже  \n",
    "- *Иду домой я слон красивый* → низкая вероятность  \n",
    "\n",
    "---\n",
    "\n",
    "## Юниграммная модель\n",
    "\n",
    "Вероятность следующего слова зависит не зависит предыдущих.  \n",
    "\n",
    "$$P(w_1, \\ldots, w_n) = \\prod_{i=1}^n P(w_i)$$\n",
    "\n",
    "\n",
    "пример: вероятность слов *the, cat, sleeps* считается независимо  \n",
    "\n",
    "---\n",
    "\n",
    "## Биграммная модель  \n",
    "\n",
    "$$P(w_1, \\ldots, w_n) = P(w_1|s) \\cdot P(w_2|w_1) \\cdot \\ldots \\cdot P(e|w_n)$$\n",
    "\n",
    "где специальные символы $s$ — начало предложения, $e$ — конец  \n",
    "\n",
    "Пример:  \n",
    "- $P($*I am*$)$ = $P(I|s) \\cdot P(am|I) \\cdot P(e|am)$\n",
    "\n",
    "---\n",
    "\n",
    "## Общий случай: N-граммная модель  \n",
    "\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = \\prod_{i=1}^{n} P(w_i \\mid w_{i-N+1} \\ldots w_{i-1})\n",
    "$$\n",
    "\n",
    "Оценка через частоты:  \n",
    "\n",
    "$$\n",
    "\\hat{P}(w_i \\mid w_{i-N+1} \\ldots w_{i-1}) = \\frac{C(w_{i-N+1} \\ldots w_{i})}{C(w_{i-N+1} \\ldots w_{i-1})}\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "## Пример: биграммы (русский)\n",
    "\n",
    "Корпус: *кошка ест рыбу*  \n",
    "\n",
    "- $P(кошка|s) = \\dfrac{C(s,кошка)}{C(s)} = 1$  \n",
    "- $P(ест|кошка) = \\dfrac{C(кошка,ест)}{C(кошка)} = 1$  \n",
    "- $P(рыбу|ест) = \\dfrac{C(ест,рыбу)}{C(ест)} = 1$  \n",
    "\n",
    "---\n",
    "\n",
    "## Проблема нулевых вероятностей  \n",
    "\n",
    "- Если биграмма **никогда не встречалась**, вероятность = 0  \n",
    "- Тогда всё предложение получает вероятность 0  \n",
    "\n",
    "  Нужны методы **сглаживания**  \n",
    "\n",
    "---\n",
    "\n",
    "## Сглаживание (Additive smoothing)\n",
    "\n",
    "$$\n",
    "\\hat{P}(w_i \\mid h) = \n",
    "\\dfrac{C(h,w_i) + d}{C(h) + d \\cdot V}\n",
    "$$  \n",
    "\n",
    "где:  \n",
    "- $h$ — контекст (n-1 слов)  \n",
    "- $V$ — размер словаря  \n",
    "- $d > 0$ — параметр сглаживания  \n",
    "\n",
    "Пример: **Laplace smoothing** ($d=1$)  \n",
    "\n",
    "---\n",
    "\n",
    "## Katz Backoff  \n",
    "\n",
    "Если частота наблюдения высокая → используем оценку по корпусу.  \n",
    "Если редкая → «откатываемся» к меньшему контексту.  \n",
    "\n",
    "$$\n",
    "\\hat{P}(w_i \\mid w_{i-N+1}^{i-1}) =\n",
    "\\begin{cases}\n",
    "\\dfrac{C(w_{i-N+1}^i)}{C(w_{i-N+1}^{i-1})} & C(w_{i-N+1}^i) > k \\\\[6pt]\n",
    "\\alpha_{w_{i-N+1}^{i-1}} \\hat{P}(w_i \\mid w_{i-N+2}^{i-1}) & \\text{иначе}\n",
    "\\end{cases}\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "## Perplexity (недоумение модели)  \n",
    "\n",
    "Мера, насколько хорошо модель предсказывает тестовый текст.  \n",
    "Чем меньше — тем лучше.  \n",
    "\n",
    "$$\n",
    "PP(\\boldsymbol{w}) = P(\\boldsymbol{w})^{-\\tfrac{1}{n}}\n",
    "$$  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Выводы  \n",
    "\n",
    "- N-граммные модели — классический способ построения языковых моделей  \n",
    "- Ограничение: учитывают только фиксированное количество слов  \n",
    "- Сглаживание необходимо для борьбы с нулевыми вероятностями  \n",
    "- Perplexity — стандартная метрика качества языковых моделей  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Актуальные проблемы в области обработки естественных языков\n",
    "\n",
    "- уменьшение и контроль \"аномалий\" (\"галлюцинаций\", hallutinations) при использовании больших языковых моделей (LLM)\n",
    "- увеличение производительности и оптимизация размера LLM\n",
    "    - квантизация\n",
    "    - дистилляция \n",
    "    - LoRA (low rank adaptation)\n",
    "- персонализация LLM\n",
    "- создание агентов на основе LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
